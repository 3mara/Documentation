%Chapter 8

\chapter{System Performance Evaluation} % Main chapter title

\label{eval} % For referencing the chapter elsewhere, use \ref{eval} 

\lhead{System Performance Evaluation. \emph{System Performance Evaluation}} % This is for the header on each page - perhaps a shortened title
%----------------------------------------------------------------------------------------
In this chapter we present out system’s performance evaluation and performance analysis. However, we need to stress again on the lack of resource for Arabic language and how that caused performance degradation of some components of the system. The chapter is organized as follows; resources are discussed from the performance point-of-view in section ~\ref{sec:challenges}, used performance measures are explained in section ~\ref{sec:tech}, while the design of the test cases is explained in section ~\ref{sec:test}. Finally, the obtained results are viewed in the last section of the chapter section ~\ref{sec:results}.

\section{Challenges}\label{sec:challenges}
This is section we give an overall discussion of the resources for Arabic language and its limitations from the perspective of performance. The main components of our system are Web Crawlers, Pre-processing tools, Similarity measure, Clustering algorithms and Recommender System. \\
We used Web Harvest as our web crawler which surfs some given URL domains and retrieve web pages’ content. Web Harvest has a good performance and a plausible throughput given the number of web pages that were crawled. \\For the Pre-processing part we used Stanford’s POS which gave a very good performance for Arabic language. We also used different Stemmers, light stemmer like the Arabic stemmer used by Lucene. However, we also used El-Khoga stemmer as different type of stemmer. All the Pre-processing components gave a plausible output and could scale-up to processing thousands of documents.\\ 
For the Similarity part, we examined different approaches; Lexical, Knowledge-based and Corpus-based similarities. For Lexical similarity, we used Lucene to build inverted index on the retrieved documents and calculate the TF-IDF. \\
Lucene is proved to be highly optimized and it gave a very good performance with processing Arabic text. For Knowledge-based approach, first we tried Arabic WordNet (AWN) to model the concepts and the knowledge of the Arabic language. \\
Unfortunately, AWN was a performance bottleneck in our system. The running time of AWN resulted in a significant degradation in the system’s performance. AWN could not scale-up to handle large data-sets. Wikipedia as ontology was our alternative to AWN. Wikipedia approach was very promising when compared to AWN, but, due to time constraints, we could not investigate this new approach as much as we did with the AWN approach. The Wikipedia’s approach scaled-up to relatively large data-sets of documents and allowed us to compute hundreds of thousands of documents similarities. Indeed, Wikipedia wasn’t computationally expensive when compared to AWN. 
Another resource that affected our performance evaluation process was the existence of a labeled training set for Arabic language. Another thing was that, we had to run the web crawlers on News websites as it’s almost the only on-line content that is written in formal Arabic language. \\

Also Arabic language is used by more than $330$ million Arabic speakers, there are few systems that we can compare our system with, also there are no human-judged dataset for Arabic text. We used Aljazeera data set \footnote[1]{Available online at http://filebox.vt.edu/users/dsaid/AljNews.tar.gz} that is categorized in to $8$ categories to measure the accurace of the different clustering algorithms.

Due to all the previously explained issues, with the addition of our hardware resources’ capabilities and the wide set of alternatives to almost each of the system’s components, we were limited by a few number of experiments. A single run might take more than 12 hours to produce an output. These, previously mentioned, reasons also limited our ability to run the clustering algorithms like Mitosis and DBSCAN with a wide range of parameters’ values to be able to tune the algorithms with the best parameter set.\\
However, we were able to test the whole system on two different randomly selected data-sets. The data-set was collected from Aljazeera news website. Some of the system techniques were tested on three different data-sets. We examined, as large as possible, different ranges of parameters for the Mitosis clustering algorithm. We also considered different performance measures like Purity and Confusion Matrix.\\


\section{Evaluation techniques}\label{sec:tech}

\section{Esperiments Design}\label{sec:test}
\section{Results and discussion}\label{sec:results}